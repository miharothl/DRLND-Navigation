#!/usr/bin/env python3
import argparse

from drl import logging
from drl.experiment.analyser import Analyzer
from drl.experiment.config import Config
from drl.experiment.experiment import Experiment
from drl.experiment.explorer import Explorer
from drl.logging import init_logging, set_logging_level, transform_verbose_count_to_logging_level
import logging
import logging.config

def parse_arguments(params):
    ap = argparse.ArgumentParser(description="Runs data prep",
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument("-v", "--verbose", dest="verbose_count",
                    action="count", default=0,
                    help="Increases log verbosity for each occurrence.")
    ap.add_argument('-t', '--train',
                    action='store_true',
                    help="Train")
    ap.add_argument('-f', '--model_filename',
                    default=None,
                    help="Model file")
    ap.add_argument('-p', '--play',
                    action='store_true',
                    help="play")
    ap.add_argument('-s', '--play_episodes',
                    type=int,
                    default=3,
                    help="number of episodes to play")
    ap.add_argument('-e', '--env',
                    default='cartpole',
                    help="Environment")
    ap.add_argument('-l', '--list_envs',
                    action='store_true',
                    help="List RL environment")
    ap.add_argument('--list_play_ex',
                    action='store_true',
                    help="List play experiments")
    ap.add_argument('--list_train_ex',
                    action='store_true',
                    help="List train experiments")
    ap.add_argument('--analyse_play_ex',
                    action='store',
                    help="Analyse play experiments")
    ap.add_argument('--analyse_compare_epoch_cols',
                    action='store',
                    choices=['avg_score', 'avg_val_score'],
                    help="Analyse train experiments")
    ap.add_argument('--exps',
                    nargs='+',
                    action='store',
                    help="Analyse train experiments")

    #                 default='score_max',
    #                 choices=['score_max', 'score_min', 'score_median', 'random'],
    #                 help="Model selection criteria")

    args = ap.parse_args()

    return args


def main():
    init_logging(logging.ERROR)

    params = Config().get_app_config()

    try:
        args = parse_arguments(params)

        set_logging_level(transform_verbose_count_to_logging_level(args.verbose_count))

        logging.debug(params)

        config = Config()
        config.set_current_env(args.env)
        environment = Experiment(config)
        # environment.set_env(args.env)
        # agent = environment.create_agent()
        # env = environment.create_env()
        explorer = Explorer(config)
        analyzer = Analyzer(config, environment.get_session_id())

        if args.list_envs:
            environment.list_envs()
        elif args.list_play_ex:
            explorer.list_play_experiments()
        elif args.list_train_ex:
            explorer.list_train_experiments()
        elif args.analyse_play_ex is not None:
            a = analyzer.play_analysis(args.analyse_play_ex)
            analyzer.log_analysis(a)
        elif args.analyse_compare_epoch_cols:
            path = analyzer.compare_train_epoch_cols(args.exps, args.analyse_compare_epoch_cols)
            print(path)
        elif args.train:
            environment.train(model=args.model_filename,
                              max_steps=config.get_train_max_steps(),
                              max_episode_steps=config.get_train_max_episodes_steps(),
                              eval_steps=config.get_train_eval_steps(),
                              eval_frequency=config.get_train_train_eval_frequency(),
                              eps_decay=config.get_train_epsilon(),
                              is_human_flag=config.get_train_is_human_flag())

            # train.dqn_rgb(agent, env)
            # train.dqn_banana(agent, env)
        elif args.play:

            if args.model_filename is not None:
                environment.play(
                    mode='human',
                    model=args.model_filename,
                    num_episodes=args.play_episodes
                )
            else:
                environment.play_dummy(
                    mode='human',
                    model=None,
                    num_episodes=args.play_episodes
                )

        # env.close()

    except Exception as e:
        logging.exception("Something went wrong :-(")

    finally:
        logging.shutdown()


if __name__ == '__main__':
    main()
